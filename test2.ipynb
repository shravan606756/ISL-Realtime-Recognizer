{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXOQ5WbJ6nh0",
        "outputId": "18a8f25a-b620-4e93-c1ba-ecd3c9d2e719"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.16.1 \\\n",
        "#                numpy==1.26.4 \\\n",
        "#                pandas \\\n",
        "#                matplotlib \\\n",
        "#                scikit-learn \\\n",
        "#                opencv-python \\\n",
        "#                mediapipe \\\n",
        "#                tqdm \\\n",
        "#                h5py \\\n",
        "#                GPUtil \\\n",
        "#                psutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install --upgrade --no-deps keras==3.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txJaTgMX6zEE"
      },
      "source": [
        "SECTION 1: IMPORTS AND SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGfux0PE61Sl",
        "outputId": "9d4b0c04-35f2-4b09-f271-211597a16aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.16.2\n",
            "GPU Available: False\n",
            "Setup completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Enable GPU memory growth and mixed precision for better performance\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "print(\"Setup completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5D1WyTM7CRc"
      },
      "source": [
        "# SECTION 2: DATASET CONFIGURATION\n",
        "# UPDATE YOUR DATASET PATH HERE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sPIFAFb7Grl",
        "outputId": "b5e60819-beea-432d-8f23-1acabc4dd19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset path: C:\\Users\\Shravan\\Downloads\\ISL_CSLRT_Corpus\\ISL_CSLRT_Corpus\n",
            "Configuration loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "DATASET_PATH = r\"C:\\Users\\Shravan\\Downloads\\ISL_CSLRT_Corpus\\ISL_CSLRT_Corpus\" # UPDATE THIS PATH!\n",
        "\n",
        "# Configuration parameters\n",
        "MAX_SEQUENCE_LENGTH = 150  # Maximum frames to process per sequence\n",
        "TARGET_SIZE = (224, 224)   # Image size for CNN processing\n",
        "BATCH_SIZE = 8             # Batch size for training\n",
        "EPOCHS = 200               # Number of training epochs\n",
        "VOCAB_SIZE = 5000          # Vocabulary size for text tokenizer\n",
        "MAX_TEXT_LENGTH = 50       # Maximum sentence length\n",
        "\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n",
        "print(f\"Configuration loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQGdvRi7NlV"
      },
      "source": [
        " SECTION 3: DATASET EXPLORER CLASS\n",
        "# This helps you understand your dataset structure before processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mnDmgEGg7Qy9"
      },
      "outputs": [],
      "source": [
        "class ISLDatasetExplorer:\n",
        "    \"\"\"Explore and understand ISL-CSLTR dataset structure\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.structure = {}\n",
        "        \n",
        "    def explore_dataset(self):\n",
        "        \"\"\"Explore the dataset structure - IMAGE FILES ONLY\"\"\"\n",
        "        print(\"=== ISL-CSLTR Dataset Structure Analysis ===\")\n",
        "        print(f\"Dataset path: {self.dataset_path}\")\n",
        "        \n",
        "        if not self.dataset_path.exists():\n",
        "            print(f\"‚ùå Error: Dataset path {self.dataset_path} does not exist!\")\n",
        "            return {}\n",
        "        \n",
        "        # Find all files and directories\n",
        "        all_items = list(self.dataset_path.rglob('*'))\n",
        "        \n",
        "        # Categorize files - REMOVED video file extensions\n",
        "        images = [f for f in all_items if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']]\n",
        "        annotations = [f for f in all_items if f.suffix.lower() in ['.json', '.csv', '.txt', '.xml']]\n",
        "        directories = [f for f in all_items if f.is_dir()]\n",
        "        \n",
        "        print(f\"\\nüìä Found:\")\n",
        "        print(f\"   Images: {len(images)}\")\n",
        "        print(f\"   Annotation files: {len(annotations)}\")\n",
        "        print(f\"   Directories: {len(directories)}\")\n",
        "        \n",
        "        # Show sample images\n",
        "        if images:\n",
        "            print(f\"\\nüñºÔ∏è Sample images:\")\n",
        "            for img in images[:5]:\n",
        "                rel_path = img.relative_to(self.dataset_path)\n",
        "                size_mb = img.stat().st_size / (1024 * 1024)\n",
        "                print(f\"   - {rel_path} ({size_mb:.1f} MB)\")\n",
        "        \n",
        "        if annotations:\n",
        "            print(f\"\\nüìÑ Annotation files:\")\n",
        "            for ann in annotations:\n",
        "                rel_path = ann.relative_to(self.dataset_path)\n",
        "                print(f\"   - {rel_path}\")\n",
        "        \n",
        "        self.structure = {\n",
        "            'images': images,\n",
        "            'annotations': annotations,\n",
        "            'directories': directories\n",
        "        }\n",
        "        \n",
        "        return self.structure\n",
        "    \n",
        "    def analyze_annotations(self):\n",
        "        \"\"\"Analyze annotation files to understand format\"\"\"\n",
        "        print(\"\\n=== Annotation Analysis ===\")\n",
        "        \n",
        "        for ann_file in self.structure.get('annotations', [])[:3]:  # Analyze first 3 files\n",
        "            print(f\"\\nüìã Analyzing: {ann_file.name}\")\n",
        "            try:\n",
        "                if ann_file.suffix.lower() == '.json':\n",
        "                    with open(ann_file, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    print(f\"   JSON structure: {type(data)}\")\n",
        "                    if isinstance(data, dict):\n",
        "                        print(f\"   Keys: {list(data.keys())[:5]}\")\n",
        "                    elif isinstance(data, list):\n",
        "                        print(f\"   List length: {len(data)}\")\n",
        "                        if data:\n",
        "                            print(f\"   Sample: {str(data[0])[:100]}...\")\n",
        "                \n",
        "                elif ann_file.suffix.lower() == '.csv':\n",
        "                    df = pd.read_csv(ann_file)\n",
        "                    print(f\"   CSV shape: {df.shape}\")\n",
        "                    print(f\"   Columns: {list(df.columns)}\")\n",
        "                    if not df.empty:\n",
        "                        print(f\"   Sample row: {df.iloc[0].to_dict()}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9akp6_ch7WLk"
      },
      "source": [
        "# ============================================================================\n",
        "# SECTION 4: RUN DATASET EXPLORATION\n",
        "# Execute this to understand your dataset structure\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pmZvaaA7Z2l",
        "outputId": "d4f288cc-d4f7-412c-c083-7ca7199e73aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ISL-CSLTR Dataset Structure Analysis ===\n",
            "Dataset path: C:\\Users\\Shravan\\Downloads\\ISL_CSLRT_Corpus\\ISL_CSLRT_Corpus\n",
            "\n",
            "üìä Found:\n",
            "   Images: 19899\n",
            "   Annotation files: 2\n",
            "   Directories: 897\n",
            "\n",
            "üñºÔ∏è Sample images:\n",
            "   - Frames_Sentence_Level\\are you free today\\1\\are you free today 01.jpg (0.0 MB)\n",
            "   - Frames_Sentence_Level\\are you free today\\1\\are you free today 02.jpg (0.0 MB)\n",
            "   - Frames_Sentence_Level\\are you free today\\1\\are you free today 03.jpg (0.0 MB)\n",
            "   - Frames_Sentence_Level\\are you free today\\1\\are you free today 04.jpg (0.0 MB)\n",
            "   - Frames_Sentence_Level\\are you free today\\1\\are you free today 05.jpg (0.0 MB)\n",
            "\n",
            "üìÑ Annotation files:\n",
            "   - ISL_CSLRT.txt\n",
            "   - corpus_csv_files\\ISL Corpus sign glosses.csv\n",
            "\n",
            "üîç Running detailed analysis...\n",
            "\n",
            "=== Annotation Analysis ===\n",
            "\n",
            "üìã Analyzing: ISL_CSLRT.txt\n",
            "\n",
            "üìã Analyzing: ISL Corpus sign glosses.csv\n",
            "   CSV shape: (101, 2)\n",
            "   Columns: ['Sentence', 'SIGN GLOSSES']\n",
            "   Sample row: {'Sentence': 'are you free today', 'SIGN GLOSSES': 'YOU FREE TODAY'}\n"
          ]
        }
      ],
      "source": [
        "explorer = ISLDatasetExplorer(DATASET_PATH)\n",
        "dataset_structure = explorer.explore_dataset()\n",
        "\n",
        "# Run detailed analysis\n",
        "if dataset_structure.get('images'):\n",
        "    print(\"\\nüîç Running detailed analysis...\")\n",
        "    explorer.analyze_annotations()\n",
        "else:\n",
        "    print(\"‚ùå No images found! Please check your dataset path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SECTION 5: DATA MANAGER CLASS\n",
        "# Handles video processing and feature extraction for folder-structured dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NWhnC0CF7jWE"
      },
      "outputs": [],
      "source": [
        "class ISLContinuousDataManager:\n",
        "    \"\"\"Data manager for ISL-CSLTR continuous sign language dataset - IMAGE FRAMES ONLY\"\"\"\n",
        "    \n",
        "    def __init__(self, base_path, max_sequence_length=150, target_size=(224, 224)):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.target_size = target_size\n",
        "        \n",
        "        # Feature dimensions\n",
        "        self.cnn_feature_dim = 1280  # MobileNetV2 features\n",
        "        self.mp_feature_dim = 1662   # MediaPipe features\n",
        "        self.total_feature_dim = self.cnn_feature_dim + self.mp_feature_dim\n",
        "        \n",
        "        # Text processing\n",
        "        self.text_tokenizer = None\n",
        "        self.vocab_size = VOCAB_SIZE\n",
        "        self.max_text_length = MAX_TEXT_LENGTH\n",
        "        \n",
        "        # Setup MediaPipe\n",
        "        self.mp_holistic = mp.solutions.holistic\n",
        "        self.mp_drawing = mp.solutions.drawing_utils\n",
        "        \n",
        "        # Setup CNN model\n",
        "        self.setup_cnn_extractor()\n",
        "        \n",
        "        # Dataset metadata\n",
        "        self.sequence_annotations = {}\n",
        "        self.signer_info = {}\n",
        "        \n",
        "        print(f\"‚úÖ ISL Data Manager initialized\")\n",
        "        print(f\"   Max sequence length: {max_sequence_length}\")\n",
        "        print(f\"   Total feature dimension: {self.total_feature_dim}\")\n",
        "    \n",
        "    def setup_cnn_extractor(self):\n",
        "        \"\"\"Setup pre-trained CNN for feature extraction\"\"\"\n",
        "        from tensorflow.keras.applications import MobileNetV2\n",
        "        \n",
        "        self.cnn_model = MobileNetV2(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=(*self.target_size, 3),\n",
        "            pooling='avg'\n",
        "        )\n",
        "        \n",
        "        # Freeze CNN layers for feature extraction\n",
        "        for layer in self.cnn_model.layers:\n",
        "            layer.trainable = False\n",
        "            \n",
        "        print(f\"   CNN feature extractor ready: {self.cnn_model.output_shape}\")\n",
        "    \n",
        "    def extract_mediapipe_features(self, results):\n",
        "        \"\"\"Extract MediaPipe keypoints from holistic results\"\"\"\n",
        "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "        return np.concatenate([pose, face, lh, rh])\n",
        "    \n",
        "    def extract_cnn_features(self, image_batch):\n",
        "        \"\"\"Extract CNN features from batch of images\"\"\"\n",
        "        processed_batch = tf.keras.applications.mobilenet_v2.preprocess_input(image_batch)\n",
        "        features = self.cnn_model(processed_batch, training=False)\n",
        "        return features.numpy()\n",
        "    \n",
        "    def load_isl_annotations(self):\n",
        "        \"\"\"Load ISL-CSLTR dataset annotations and metadata\"\"\"\n",
        "        print(\"üìñ Loading ISL-CSLTR annotations...\")\n",
        "        \n",
        "        # Look for annotation files\n",
        "        annotation_patterns = [\n",
        "            '*.json', '*.csv', '*annotations*', '*labels*', '*metadata*', \n",
        "            '*sentences*', '*glosses*', '*translations*'\n",
        "        ]\n",
        "        \n",
        "        annotation_files = []\n",
        "        for pattern in annotation_patterns:\n",
        "            annotation_files.extend(self.base_path.rglob(pattern))\n",
        "        \n",
        "        # Remove duplicates\n",
        "        annotation_files = list(set(annotation_files))\n",
        "        print(f\"   Found {len(annotation_files)} potential annotation files\")\n",
        "        \n",
        "        annotations = {}\n",
        "        \n",
        "        for ann_file in annotation_files:\n",
        "            try:\n",
        "                print(f\"   Processing {ann_file.name}...\")\n",
        "                \n",
        "                if ann_file.suffix.lower() == '.json':\n",
        "                    with open(ann_file, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    \n",
        "                    # Handle different JSON structures\n",
        "                    if isinstance(data, dict):\n",
        "                        for key, value in data.items():\n",
        "                            if isinstance(value, dict) and any(field in value for field in ['sentence', 'translation', 'gloss', 'text']):\n",
        "                                annotations[key] = value\n",
        "                            elif isinstance(value, str) and len(value.split()) > 1:\n",
        "                                annotations[key] = {'sentence': value, 'text': value}\n",
        "                    \n",
        "                    elif isinstance(data, list):\n",
        "                        for item in data:\n",
        "                            if isinstance(item, dict):\n",
        "                                sequence_id = item.get('sequence_id') or item.get('folder') or item.get('id')\n",
        "                                if sequence_id:\n",
        "                                    annotations[sequence_id] = item\n",
        "                \n",
        "                elif ann_file.suffix.lower() == '.csv':\n",
        "                    df = pd.read_csv(ann_file)\n",
        "                    \n",
        "                    # Find relevant columns\n",
        "                    sequence_cols = [col for col in df.columns if any(term in col.lower() for term in ['sequence', 'folder', 'name', 'id'])]\n",
        "                    text_cols = [col for col in df.columns if any(term in col.lower() for term in ['sentence', 'translation', 'gloss', 'text'])]\n",
        "                    signer_cols = [col for col in df.columns if 'signer' in col.lower()]\n",
        "                    \n",
        "                    if sequence_cols and text_cols:\n",
        "                        sequence_col = sequence_cols[0]\n",
        "                        text_col = text_cols[0]\n",
        "                        signer_col = signer_cols[0] if signer_cols else None\n",
        "                        \n",
        "                        for _, row in df.iterrows():\n",
        "                            sequence_id = str(row[sequence_col])\n",
        "                            text = str(row[text_col])\n",
        "                            \n",
        "                            annotation = {\n",
        "                                'sentence': text,\n",
        "                                'text': text,\n",
        "                                'translation': text\n",
        "                            }\n",
        "                            \n",
        "                            if signer_col:\n",
        "                                annotation['signer'] = str(row[signer_col])\n",
        "                            \n",
        "                            annotations[sequence_id] = annotation\n",
        "                \n",
        "                elif ann_file.suffix.lower() == '.txt':\n",
        "                    with open(ann_file, 'r', encoding='utf-8') as f:\n",
        "                        lines = f.readlines()\n",
        "                    \n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if ':' in line and len(line.split(':')) == 2:\n",
        "                            sequence_id, text = line.split(':', 1)\n",
        "                            annotations[sequence_id.strip()] = {\n",
        "                                'sentence': text.strip(),\n",
        "                                'text': text.strip()\n",
        "                            }\n",
        "                        elif '\\t' in line:\n",
        "                            parts = line.split('\\t')\n",
        "                            if len(parts) >= 2:\n",
        "                                sequence_id, text = parts[0], parts[1]\n",
        "                                annotations[sequence_id.strip()] = {\n",
        "                                    'sentence': text.strip(),\n",
        "                                    'text': text.strip()\n",
        "                                }\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error processing {ann_file}: {e}\")\n",
        "        \n",
        "        self.sequence_annotations = annotations\n",
        "        print(f\"   ‚úÖ Loaded annotations for {len(annotations)} sequences\")\n",
        "        \n",
        "        # Extract signer information\n",
        "        signers = set()\n",
        "        for ann in annotations.values():\n",
        "            if isinstance(ann, dict) and 'signer' in ann:\n",
        "                signers.add(ann['signer'])\n",
        "        \n",
        "        if signers:\n",
        "            self.signer_info = {signer: idx for idx, signer in enumerate(sorted(signers))}\n",
        "            print(f\"   Found {len(signers)} signers: {list(signers)}\")\n",
        "        \n",
        "        return annotations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H9F3wLl7q-l"
      },
      "source": [
        "# SECTION 6: INITIALIZE DATA MANAGER\n",
        "# Create the data manager instance for folder-structured dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ0t9j-97rwc",
        "outputId": "a985393b-0e3a-4c77-c324-ab6f7d796b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Initializing ISL Data Manager...\n",
            "   CNN feature extractor ready: (None, 1280)\n",
            "‚úÖ ISL Data Manager initialized\n",
            "   Max sequence length: 150\n",
            "   Total feature dimension: 2942\n",
            "üìñ Loading ISL-CSLTR annotations...\n",
            "   Found 4 potential annotation files\n",
            "   Processing sentences.pickle...\n",
            "   Processing metadata.pickle...\n",
            "   Processing ISL Corpus sign glosses.csv...\n",
            "   Processing processing_params.json...\n",
            "   ‚úÖ Loaded annotations for 0 sequences\n",
            "‚úÖ Data manager initialized with 0 annotations\n"
          ]
        }
      ],
      "source": [
        "# Initialize the data manager\n",
        "print(\"üöÄ Initializing ISL Data Manager...\")\n",
        "data_manager = ISLContinuousDataManager(\n",
        "    base_path=DATASET_PATH,\n",
        "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    target_size=TARGET_SIZE\n",
        ")\n",
        "\n",
        "# Load annotations\n",
        "annotations = data_manager.load_isl_annotations()\n",
        "print(f\"‚úÖ Data manager initialized with {len(annotations)} annotations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn-sEo1e95dN"
      },
      "source": [
        "# SECTION 7: IMAGE SEQUENCE PROCESSING FUNCTIONS\n",
        "# Functions to process image frame sequences and extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OAg30mZH-Nkt"
      },
      "outputs": [],
      "source": [
        "def find_image_sequences(base_path):\n",
        "    \"\"\"\n",
        "    Find all image sequences organized in folders.\n",
        "    Assumes structure like: base_path/sequence_name/frame_001.jpg, frame_002.jpg, etc.\n",
        "    \"\"\"\n",
        "    base_path = Path(base_path)\n",
        "    sequences = {}\n",
        "    \n",
        "    print(f\"   Searching for image sequences in: {base_path}\")\n",
        "    \n",
        "    # Find all directories that contain images\n",
        "    all_dirs = [d for d in base_path.rglob('*') if d.is_dir()]\n",
        "    print(f\"   Found {len(all_dirs)} directories to check\")\n",
        "    \n",
        "    for folder in all_dirs:\n",
        "        # Get all images in this folder\n",
        "        images = sorted([\n",
        "            f for f in folder.iterdir() \n",
        "            if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']\n",
        "        ])\n",
        "        \n",
        "        if images:\n",
        "            sequence_name = folder.name\n",
        "            sequences[sequence_name] = images\n",
        "            print(f\"   ‚úì Sequence '{sequence_name}': {len(images)} frames\")\n",
        "    \n",
        "    print(f\"   Total sequences found: {len(sequences)}\")\n",
        "    return sequences\n",
        "\n",
        "def process_image_sequence(data_manager, image_paths, annotation=None, signer_id=None):\n",
        "    \"\"\"\n",
        "    Process a sequence of image frames for sign language recognition.\n",
        "    REPLACES: process_continuous_video() - now works with image lists instead of video files\n",
        "    \"\"\"\n",
        "    total_frames = len(image_paths)\n",
        "    \n",
        "    if total_frames == 0:\n",
        "        print(f\"‚ö†Ô∏è Warning: No frames in sequence\")\n",
        "        return None\n",
        "    \n",
        "    # Handle variable length sequences\n",
        "    if total_frames > data_manager.max_sequence_length:\n",
        "        # Sample frames uniformly\n",
        "        frame_indices = np.linspace(0, total_frames-1, data_manager.max_sequence_length, dtype=int)\n",
        "        selected_images = [image_paths[i] for i in frame_indices]\n",
        "    else:\n",
        "        selected_images = image_paths\n",
        "    \n",
        "    # Read selected frames\n",
        "    frames = []\n",
        "    for img_path in selected_images:\n",
        "        try:\n",
        "            frame = cv2.imread(str(img_path))\n",
        "            if frame is not None:\n",
        "                frames.append(frame)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Warning: Could not read {img_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error reading {img_path}: {e}\")\n",
        "        \n",
        "        if len(frames) >= data_manager.max_sequence_length:\n",
        "            break\n",
        "    \n",
        "    if len(frames) == 0:\n",
        "        print(f\"‚ö†Ô∏è Warning: No frames successfully loaded\")\n",
        "        return None\n",
        "    \n",
        "    # Process frames with MediaPipe and CNN\n",
        "    mp_features = []\n",
        "    cnn_images = []\n",
        "    \n",
        "    with data_manager.mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        for frame in frames:\n",
        "            # MediaPipe processing\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = holistic.process(frame_rgb)\n",
        "            mp_feat = data_manager.extract_mediapipe_features(results)\n",
        "            mp_features.append(mp_feat)\n",
        "            \n",
        "            # Prepare image for CNN\n",
        "            img_resized = cv2.resize(frame, data_manager.target_size)\n",
        "            cnn_images.append(img_resized)\n",
        "    \n",
        "    # Extract CNN features in batch\n",
        "    cnn_batch = np.array(cnn_images, dtype=np.float32)\n",
        "    cnn_features = data_manager.extract_cnn_features(cnn_batch)\n",
        "    \n",
        "    # Combine features\n",
        "    hybrid_sequence = []\n",
        "    actual_length = len(frames)\n",
        "    \n",
        "    for i in range(len(frames)):\n",
        "        combined = np.concatenate([cnn_features[i], mp_features[i]])\n",
        "        hybrid_sequence.append(combined)\n",
        "    \n",
        "    # Pad sequence to max_sequence_length if necessary\n",
        "    while len(hybrid_sequence) < data_manager.max_sequence_length:\n",
        "        hybrid_sequence.append(np.zeros(data_manager.total_feature_dim))\n",
        "    \n",
        "    return {\n",
        "        'features': np.array(hybrid_sequence[:data_manager.max_sequence_length]),\n",
        "        'sentence': annotation.get('sentence', '') if annotation else '',\n",
        "        'text': annotation.get('text', '') if annotation else '',\n",
        "        'signer': signer_id,\n",
        "        'actual_length': actual_length,\n",
        "        'sequence_name': image_paths[0].parent.name if image_paths else 'unknown'\n",
        "    }\n",
        "\n",
        "def process_isl_dataset(data_manager, save_processed=True):\n",
        "    \"\"\"\n",
        "    Process the entire ISL-CSLTR dataset from image sequences.\n",
        "    UPDATED: Now processes image folders instead of video files\n",
        "    \"\"\"\n",
        "    print(\"üé¨ Processing ISL-CSLTR Dataset (Image Sequences)...\")\n",
        "    \n",
        "    # First, let's check what's in the dataset path\n",
        "    print(f\"   Scanning dataset path: {data_manager.base_path}\")\n",
        "    \n",
        "    # Find all image sequences\n",
        "    sequences = find_image_sequences(data_manager.base_path)\n",
        "    \n",
        "    if not sequences:\n",
        "        print(\"‚ùå No image sequences found! Please check your dataset structure.\")\n",
        "        print(\"Expected structure: base_path/sequence_name/frame_001.jpg, frame_002.jpg, etc.\")\n",
        "        print(\"\\nTrying alternative: Looking for loose images...\")\n",
        "        \n",
        "        # Alternative: check if images are directly in base_path without subdirectories\n",
        "        all_images = sorted([\n",
        "            f for f in data_manager.base_path.rglob('*')\n",
        "            if f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']\n",
        "        ])\n",
        "        \n",
        "        if all_images:\n",
        "            print(f\"   Found {len(all_images)} loose image files\")\n",
        "            print(f\"   Sample paths: {[str(img.relative_to(data_manager.base_path)) for img in all_images[:3]]}\")\n",
        "            \n",
        "            # Group by parent directory\n",
        "            from collections import defaultdict\n",
        "            sequences_dict = defaultdict(list)\n",
        "            for img in all_images:\n",
        "                parent_name = img.parent.name if img.parent != data_manager.base_path else 'root'\n",
        "                sequences_dict[parent_name].append(img)\n",
        "            \n",
        "            sequences = dict(sequences_dict)\n",
        "            print(f\"   Organized into {len(sequences)} sequences by parent folder\")\n",
        "        else:\n",
        "            print(\"   No images found at all in the dataset path!\")\n",
        "            return [], None\n",
        "    \n",
        "    print(f\"   Found {len(sequences)} image sequences\")\n",
        "    \n",
        "    # ADDED: Show annotation matching info\n",
        "    print(f\"\\nüìã Annotation Matching:\")\n",
        "    print(f\"   Available annotations: {len(data_manager.sequence_annotations)}\")\n",
        "    if data_manager.sequence_annotations:\n",
        "        print(f\"   Sample annotation keys: {list(data_manager.sequence_annotations.keys())[:5]}\")\n",
        "    print(f\"   Sample sequence names: {list(sequences.keys())[:5]}\")\n",
        "    \n",
        "    processed_data = []\n",
        "    all_texts = []\n",
        "    \n",
        "    # ADDED: Track annotation matching\n",
        "    matched_count = 0\n",
        "    unmatched_sequences = []\n",
        "    \n",
        "    # Process each sequence with progress bar\n",
        "    for sequence_name, image_paths in tqdm(sequences.items(), desc=\"Processing sequences\"):\n",
        "        try:\n",
        "            # Find annotation for this sequence\n",
        "            annotation = None\n",
        "            signer_id = None\n",
        "            \n",
        "            # IMPROVED: Try multiple matching strategies\n",
        "            # Strategy 1: Direct match\n",
        "            if sequence_name in data_manager.sequence_annotations:\n",
        "                annotation = data_manager.sequence_annotations[sequence_name]\n",
        "                matched_count += 1\n",
        "            \n",
        "            # Strategy 2: Try with parent folder name\n",
        "            if not annotation and image_paths:\n",
        "                parent_folder = image_paths[0].parent.parent.name\n",
        "                if parent_folder in data_manager.sequence_annotations:\n",
        "                    annotation = data_manager.sequence_annotations[parent_folder]\n",
        "                    matched_count += 1\n",
        "            \n",
        "            # Strategy 3: Try case-insensitive match\n",
        "            if not annotation:\n",
        "                for key in data_manager.sequence_annotations.keys():\n",
        "                    if key.lower() == sequence_name.lower():\n",
        "                        annotation = data_manager.sequence_annotations[key]\n",
        "                        matched_count += 1\n",
        "                        break\n",
        "            \n",
        "            # Strategy 4: Try numeric match (e.g., '001' matches '1')\n",
        "            if not annotation and sequence_name.isdigit():\n",
        "                for key in data_manager.sequence_annotations.keys():\n",
        "                    if key.lstrip('0') == sequence_name.lstrip('0'):\n",
        "                        annotation = data_manager.sequence_annotations[key]\n",
        "                        matched_count += 1\n",
        "                        break\n",
        "            \n",
        "            if not annotation:\n",
        "                unmatched_sequences.append(sequence_name)\n",
        "            \n",
        "            if isinstance(annotation, dict):\n",
        "                signer_id = annotation.get('signer')\n",
        "            \n",
        "            # Process image sequence\n",
        "            result = process_image_sequence(data_manager, image_paths, annotation, signer_id)\n",
        "            \n",
        "            if result is not None:\n",
        "                processed_data.append(result)\n",
        "                if result['text']:\n",
        "                    all_texts.append(result['text'])\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {sequence_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # ADDED: Show matching statistics\n",
        "    print(f\"\\nüìä Annotation Matching Results:\")\n",
        "    print(f\"   Sequences with annotations: {matched_count}/{len(sequences)}\")\n",
        "    print(f\"   Sequences without annotations: {len(unmatched_sequences)}\")\n",
        "    if unmatched_sequences[:5]:\n",
        "        print(f\"   Sample unmatched: {unmatched_sequences[:5]}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Successfully processed {len(processed_data)} sequences\")\n",
        "    print(f\"   Sequences with text: {len(all_texts)}\")\n",
        "    \n",
        "    # Setup text tokenizer\n",
        "    if all_texts:\n",
        "        setup_text_tokenizer(data_manager, all_texts)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: No text annotations found! Model will train without text labels.\")\n",
        "        print(\"   Check that your annotation file format matches the sequence names.\")\n",
        "    \n",
        "    # Save processed data\n",
        "    if save_processed:\n",
        "        save_path = save_processed_data(data_manager, processed_data)\n",
        "        return processed_data, save_path\n",
        "    \n",
        "    return processed_data, None\n",
        "\n",
        "def setup_text_tokenizer(data_manager, text_data):\n",
        "    \"\"\"Setup tokenizer for sentence annotations\"\"\"\n",
        "    # FIXED: Updated import for TensorFlow 2.x compatibility\n",
        "    try:\n",
        "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    except ImportError:\n",
        "        try:\n",
        "            from keras.preprocessing.text import Tokenizer\n",
        "        except ImportError:\n",
        "            # For TensorFlow 2.16+, use keras_preprocessing\n",
        "            from keras_preprocessing.text import Tokenizer\n",
        "    \n",
        "    print(\"üìù Setting up text tokenizer...\")\n",
        "    \n",
        "    # Clean text data\n",
        "    cleaned_texts = []\n",
        "    for text in text_data:\n",
        "        if isinstance(text, str) and text.strip():\n",
        "            cleaned = text.lower().strip()\n",
        "            cleaned = ''.join(c if c.isalnum() or c.isspace() or c in '.,!?' else ' ' for c in cleaned)\n",
        "            cleaned_texts.append(cleaned)\n",
        "    \n",
        "    if not cleaned_texts:\n",
        "        print(\"‚ö†Ô∏è No valid text data found for tokenizer\")\n",
        "        return\n",
        "    \n",
        "    # Create tokenizer\n",
        "    data_manager.text_tokenizer = Tokenizer(\n",
        "        num_words=data_manager.vocab_size,\n",
        "        oov_token='<OOV>',\n",
        "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    )\n",
        "    \n",
        "    data_manager.text_tokenizer.fit_on_texts(cleaned_texts)\n",
        "    print(f\"   ‚úÖ Tokenizer created with {len(data_manager.text_tokenizer.word_index)} unique words\")\n",
        "    \n",
        "    # Save tokenizer\n",
        "    with open(data_manager.base_path / 'isl_text_tokenizer.pickle', 'wb') as f:\n",
        "        pickle.dump(data_manager.text_tokenizer, f)\n",
        "\n",
        "def save_processed_data(data_manager, processed_data):\n",
        "    \"\"\"\n",
        "    Save processed ISL-CSLTR features and metadata.\n",
        "    UPDATED: Changed 'video_paths' to 'sequence_names' to reflect image-based processing\n",
        "    \"\"\"\n",
        "    save_path = data_manager.base_path / 'processed_isl_features'\n",
        "    save_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    print(f\"üíæ Saving processed data to {save_path}\")\n",
        "    \n",
        "    # Separate data components\n",
        "    features = []\n",
        "    texts = []\n",
        "    sentences = []\n",
        "    signers = []\n",
        "    lengths = []\n",
        "    sequence_names = []\n",
        "    \n",
        "    for item in processed_data:\n",
        "        features.append(item['features'])\n",
        "        texts.append(item['text'] if item['text'] else '')\n",
        "        sentences.append(item['sentence'] if item['sentence'] else '')\n",
        "        signers.append(item['signer'] if item['signer'] else 'unknown')\n",
        "        lengths.append(item['actual_length'])\n",
        "        sequence_names.append(item['sequence_name'])\n",
        "    \n",
        "    # Save arrays and metadata\n",
        "    np.save(save_path / 'sequence_features.npy', np.array(features))\n",
        "    \n",
        "    with open(save_path / 'texts.pickle', 'wb') as f:\n",
        "        pickle.dump(texts, f)\n",
        "    \n",
        "    with open(save_path / 'sentences.pickle', 'wb') as f:\n",
        "        pickle.dump(sentences, f)\n",
        "    \n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'signers': signers,\n",
        "        'sequence_lengths': lengths,\n",
        "        'sequence_names': sequence_names,\n",
        "        'signer_info': data_manager.signer_info\n",
        "    }\n",
        "    \n",
        "    with open(save_path / 'metadata.pickle', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "    \n",
        "    # Save parameters\n",
        "    params = {\n",
        "        'max_sequence_length': data_manager.max_sequence_length,\n",
        "        'target_size': data_manager.target_size,\n",
        "        'total_feature_dim': data_manager.total_feature_dim,\n",
        "        'num_samples': len(processed_data),\n",
        "        'num_signers': len(data_manager.signer_info),\n",
        "        'vocab_size': data_manager.vocab_size\n",
        "    }\n",
        "    \n",
        "    with open(save_path / 'processing_params.json', 'w') as f:\n",
        "        json.dump(params, f, indent=2)\n",
        "    \n",
        "    print(f\"   ‚úÖ Saved {len(processed_data)} samples\")\n",
        "    print(f\"   Features shape: {np.array(features).shape}\")\n",
        "    print(f\"   Unique signers: {len(set(signers))}\")\n",
        "    print(f\"   Text samples: {len([t for t in texts if t])}\")\n",
        "    \n",
        "    return save_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsopA6NC-dFm"
      },
      "source": [
        "# SECTION 8: PROCESS DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "D2T5y67Z_gzf",
        "outputId": "9f3ba28b-6db8-4d36-ef06-c2b2af45a469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Found existing processed data. Loading...\n",
            "‚úÖ Loaded 121 processed samples\n",
            "   Feature shape: (121, 150, 2942)\n"
          ]
        }
      ],
      "source": [
        "# Check if processed data already exists\n",
        "processed_path = Path(DATASET_PATH) / 'processed_isl_features'\n",
        "\n",
        "if processed_path.exists():\n",
        "    print(\"üìÇ Found existing processed data. Loading...\")\n",
        "    \n",
        "    # Load existing processed data\n",
        "    features = np.load(processed_path / 'sequence_features.npy')\n",
        "    \n",
        "    with open(processed_path / 'texts.pickle', 'rb') as f:\n",
        "        texts = pickle.load(f)\n",
        "    \n",
        "    with open(processed_path / 'metadata.pickle', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer_path = Path(DATASET_PATH) / 'isl_text_tokenizer.pickle'\n",
        "    if tokenizer_path.exists():\n",
        "        with open(tokenizer_path, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        tokenizer = None\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(features)} processed samples\")\n",
        "    print(f\"   Feature shape: {features.shape}\")\n",
        "    \n",
        "else:\n",
        "    print(\"üé¨ Processing dataset for the first time...\")\n",
        "    print(\"‚ö†Ô∏è This may take several hours depending on dataset size!\")\n",
        "    \n",
        "    # Process the dataset\n",
        "    processed_data, save_path = process_isl_dataset(data_manager, save_processed=True)\n",
        "    \n",
        "    # Extract processed components\n",
        "    features = np.array([item['features'] for item in processed_data])\n",
        "    texts = [item['text'] for item in processed_data]\n",
        "    metadata = {\n",
        "        'signers': [item['signer'] for item in processed_data],\n",
        "        'sequence_lengths': [item['actual_length'] for item in processed_data],\n",
        "        'sequence_names': [item['sequence_name'] for item in processed_data]\n",
        "    }\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer_path = Path(DATASET_PATH) / 'isl_text_tokenizer.pickle'\n",
        "    if tokenizer_path.exists():\n",
        "        with open(tokenizer_path, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "    else:\n",
        "        tokenizer = None\n",
        "    \n",
        "    print(\"‚úÖ Dataset processing completed!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-gpu (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
